# -*- coding: utf-8 -*-
"""digit_recognization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JQWWsfqaQjlTv28RyTIgqDpUUFIvuS9I
"""

from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', version=1, as_frame=False)
mnist.keys()

X, y = mnist['data'], mnist['target']
X.shape, y.shape

import numpy as np
y = y.astype(np.int64)
y

"""### Preprocessing"""

X_9s = X[y == 9].reshape(6958, 28, 28)

import matplotlib.pyplot as plt

plt.matshow(X_9s[0], cmap = plt.cm.binary)

for i in range(len(X_9s)):
    X_9s[i] = np.r_[X_9s[i][2:], np.zeros((2, 28))]

plt.matshow(X_9s[0], cmap=plt.cm.binary)

X[y == 9] = X_9s.reshape(6958, 784)

# Standard Scaler
from sklearn.preprocessing import StandardScaler

std = StandardScaler()

X_train, X_val, X_test = X[:60000], X[60000:65000], X[65000:]
y_train, y_val, y_test = y[:60000], y[60000:65000], y[65000:]

X_train_scaled = std.fit_transform(X_train)
X_val_scaled = std.transform(X_val)
X_test_scaled = std.transform(X_test)

"""### Gradient descent for softmax regression"""

## Bias trick

X_train_with_bias = np.c_[np.ones((len(X_train), 1)), X_train_scaled]
X_val_with_bias = np.c_[np.ones((len(X_val), 1)), X_val_scaled]
X_test_with_bias = np.c_[np.ones((len(X_test), 1)), X_test_scaled]
X_train_with_bias.shape, X_val_with_bias.shape, X_test_with_bias.shape

def convert_to_one_hot(labels):
    n_classes = np.max(labels) + 1
    m = len(labels)
    one_hot = np.zeros((m, n_classes))
    for i in range(m):
        one_hot[i][labels[i]] = 1
    return one_hot

Y_train_one_hot = convert_to_one_hot(y_train)
Y_val_one_hot = convert_to_one_hot(y_val)
Y_test_one_hot = convert_to_one_hot(y_test)

def softmax(logits):
    exps = np.exp(logits)
    return exps / np.sum(exps, axis=1, keepdims=True)

n_inputs = X_train.shape[1] + 1
n_outputs = len(np.unique(y))
n_inputs, n_outputs

# # EARLY STOPPING
# np.random.seed(25)
# eta = 0.1
# n_iterations = 10001
# m = len(X_train)
# epsilon = 1e-7
# best_loss = np.infty

# Theta_et = np.random.randn(n_inputs, n_outputs)

# for it in range(n_iterations):
#     logits = X_train_with_bias.dot(Theta_et)
#     Y_proba = softmax(logits)
#     error = Y_proba - Y_train_one_hot
#     gradients = 1/m * X_train_with_bias.T.dot(error) 
#     Theta_et = Theta_et - eta * gradients

#     logits = X_val_with_bias.dot(Theta_et)
#     Y_proba = softmax(logits)
#     loss = -np.mean(np.sum(Y_val_one_hot * np.log(Y_proba + epsilon), axis=1))

#     if it % 50 == 0:
#         print(it, loss)
#     if loss < best_loss:
#         best_loss = loss
#     else:
#         print(it - 1, best_loss)
#         print(it, loss, 'Early stopping')
#         break

# Mini batch gradient descent
# np.random.seed(25)
eta = 0.1
n_epochs = 100
minibatch_size = 30
m = len(X_train)
epsilon = 1e-7
loss_mgd = []

#theta = np.random.rand(n_inputs, n_outputs)

for epoch in range(n_epochs):
    random_indices = np.random.permutation(m)
    X_shuffle = X_train_with_bias[random_indices]
    y_shuffle = Y_train_one_hot[random_indices]
    for i in range(0, m, minibatch_size):
        xi = X_shuffle[i:i+minibatch_size]
        yi = y_shuffle[i:i+minibatch_size]
        logits = xi.dot(theta)
        Y_proba = softmax(logits)

        loss = -np.mean(np.sum(yi * np.log(Y_proba + epsilon), axis=1))
        #if i % 3000 == 0:
           #print("Loss: ", loss)
        loss_mgd.append(loss)

        error = Y_proba - yi
        gradients = 1/m * xi.T.dot(error)
        theta = theta - eta*gradients

plt.plot([i for i in range(len(loss_mgd))], loss_mgd)
plt.axis([199990, 200000, 0, 3])

# Evaluation

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_recall_fscore_support as score
def display_score(precision, recall, f1_score):
    score = np.array([[str(i) for i in range(10)], np.round(precision*100, 2), np.round(recall*100, 2), np.round(f1_score*100, 2)])
    df = pd.DataFrame(data=score.T, columns=['Labels', 'Precision', 'Recall', 'F1_score'])
    return df.set_index('Labels')

# on training set
logits = X_train_with_bias.dot(theta)
Y_proba = softmax(logits)
y_pred = np.argmax(Y_proba, axis=1)
precision, recall, f1score, sp = score(y_train, y_pred)
display_score(precision, recall, f1score)

accuracy_score(y_train, y_pred)

# on validation set
logits = X_val_with_bias.dot(theta)
Y_proba = softmax(logits)
y_pred = np.argmax(Y_proba, axis=1)
precision, recall, f1score, sp = score(y_val, y_pred)
display_score(precision, recall, f1score)

accuracy_score(y_val, y_pred)

# on test set
logits = X_test_with_bias.dot(theta)
Y_proba = softmax(logits)
y_pred = np.argmax(Y_proba, axis=1)
precision, recall, f1score, sp = score(y_test, y_pred)
display_score(precision, recall, f1score)

accuracy_score(y_test, y_pred)

import joblib
joblib.dump(std, 'std.pkl')

"""#### Save and download weights"""

with open('weights_mgd.txt', 'w') as f:
    for i in theta:
        for j in i:
            f.write(str(j) + " ")

# Theta_et = []
# with open('/content/drive/MyDrive/weights_et.txt', 'r') as f:
#     weights = [float(i) for i in f.read().split()]

# for i in range(0, len(weights), 10):
#     Theta_et.append(weights[i:i+10])

# Theta_et = np.array(Theta_et)
# print(Theta_et.shape)

"""### USE KNN"""

from sklearn import neighbors
from sklearn.metrics import confusion_matrix


clf_knn = neighbors.KNeighborsClassifier(n_neighbors = 3, p = 2) # K = 3, accuracy_score = 0.9705
clf_knn.fit(X_train_scaled, y_train)

y_pred_knn = clf_knn.predict(X_train_scaled)
precision, recall, f1_score, support = score(y_train, y_pred_knn)
display_score(precision, recall, f1_score)

accuracy_score(y_train, y_pred_knn)

# valid set
y_pred_knn = clf_knn.predict(X_val_scaled)
precision, recall, f1_score, support = score(y_val, y_pred_knn)
display_score(precision, recall, f1_score)

accuracy_score(y_val, y_pred_knn)

# test set
y_pred_knn = clf_knn.predict(X_test_scaled)
precision, recall, f1_score, support = score(y_test, y_pred_knn)
display_score(precision, recall, f1_score)

accuracy_score(y_test, y_pred_knn)

"""### USE SKLEARN (Logistic Regression)

"""

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=1, max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# on training set
y_pred_log = log_reg.predict(X_train_scaled)
precision, recall, f1_score, support= score(y_train, y_pred_log)
display_score(precision, recall, f1_score)

accuracy_score(y_train, y_pred_log)

# on valid set
y_pred_log = log_reg.predict(X_val_scaled)
precision, recall, f1_score, support= score(y_val, y_pred_log)
display_score(precision, recall, f1_score)

accuracy_score(y_val, y_pred_log)

# on test set
y_pred_log = log_reg.predict(X_test_scaled)
precision, recall, f1_score, support= score(y_test, y_pred_log)
display_score(precision, recall, f1_score)

accuracy_score(y_test, y_pred_log)

"""### SVM"""

from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import SGDClassifier

# train with LinearSVC first
lin_clf = LinearSVC(random_state=25, max_iter=10000)
lin_clf.fit(X_train_scaled, y_train)

y_pred = lin_clf.predict(X_train_scaled)
accuracy_score(y_train, y_pred)

y_pred = lin_clf.predict(X_val_scaled)
accuracy_score(y_val, y_pred)

# use SVC with kernel rbf
svm_clf = SVC(kernel='rbf', probability=True)
svm_clf.fit(X_train_scaled[:10000], y_train[:10000])

y_pred = svm_clf.predict(X_train_scaled)
accuracy_score(y_train, y_pred)

y_pred = svm_clf.predict(X_val_scaled)
accuracy_score(y_val, y_pred)

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal, uniform

param_distributions = {
    'gamma': reciprocal(0.001, 0.1),
    'C': uniform(1, 10)
}

rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=1000, cv=3, verbose=2)
rnd_search_cv.fit(X_train_scaled[:2000], y_train[:2000])

rnd_search_cv.best_estimator_

rnd_search_cv.best_score_

rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)

y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)
accuracy_score(y_train, y_pred)

y_pred = rnd_search_cv.best_estimator_.predict(X_val_scaled)
accuracy_score(y_val, y_pred)

y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)
accuracy_score(y_test, y_pred)

precision, recall, f1_score, sp = score(y_test, y_pred)
display_score(precision, recall, f1_score)

best_model = rnd_search_cv.best_estimator_
joblib.dump(best_model, 'svm_clf.pkl')

